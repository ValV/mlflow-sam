{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Segment Anything\n",
    "\n",
    "Segment Anything MLflow model logging example.\n",
    "\n",
    "## Setup environment\n",
    "\n",
    "Conda: Python 3.12.\n",
    "\n",
    "### Conda\n",
    "\n",
    "Basically only Python is needed. Additionally install MLflow with Conda and Boto3 for S3 bucket access.\n",
    "\n",
    "```shell\n",
    "conda create -n mlflow-x -c conda-forge python==3.12 mlflow==2.17.0 boto3 python-dotenv\n",
    "conda activate mlflow-x\n",
    "```\n",
    "\n",
    "Additionally install JupyterLab for experiments:\n",
    "```shell\n",
    "conda install jupyterlab nb_conda\n",
    "```\n",
    "\n",
    "### Pip\n",
    "\n",
    "PyTorch CPU version (GPU version is not so tricky):\n",
    "\n",
    "```shell\n",
    "pip install -f https://download.pytorch.org/whl/torch torch==2.5.0+cpu\n",
    "pip install -f https://download.pytorch.org/whl/torchvision torchvision==0.20.0+cpu\n",
    "```\n",
    "\n",
    "Segment Anything:\n",
    "\n",
    "```shell\n",
    "pip install git+https://github.com/facebookresearch/segment-anything.git@6fdee8f2727f4506cfbbe553e23b895e27956588\n",
    "```\n",
    "\n",
    "OpenCV:\n",
    "\n",
    "```shell\n",
    "pip install opencv-python-headless\n",
    "```\n",
    "\n",
    "### Jupyter Lab\n",
    "\n",
    "This notebook can be successfully launched in Jupyter Lab.\n",
    "\n",
    "> NOTE: Jupyter cells may fail on timeout, run with increased limits, e. g. `jupyter lab --ServerApp.rate_limit_window=1440.0`\n",
    "\n",
    "### Variables in .env\n",
    "\n",
    "Python dotenv is used to set environment variables from `.env` that must reside in the notebook's working directory.\n",
    "\n",
    "> Nobody sets environment variables in notebooks, right ðŸ˜‰\n",
    "\n",
    "Mandatory variables (assume MLflow tracking server is a separate host):\n",
    "\n",
    "```shell\n",
    "MLFLOW_TRACKING_URI=http://host-or-ip:5000\n",
    "```\n",
    "\n",
    "Optional variables (assume MLflow artifact storage is configured as a separate S3 storage):\n",
    "```shell\n",
    "MLFLOW_S3_ENDPOINT_URL=\"https://storage.host.name\"\n",
    "AWS_ACCESS_KEY_ID=\"s3-key-id\"\n",
    "AWS_SECRET_ACCESS_KEY=\"s3-key-secret\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from os import environ\n",
    "from traceback import format_exc\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()  # take environment variables from .env (not overwrite existing)\n",
    "\n",
    "# MLflow tracking host address: empty string defaults to ./mlruns directory\n",
    "MLFLOW_TRACKING_URI = environ.get('MLFLOW_TRACKING_URI', '')  # or 'http://localhost:5000'\n",
    "# assert MLFLOW_TRACKING_URI, f\"Provide MLFLOW_TRACKING_URI environment variable or set above!\"\n",
    "\n",
    "PATH_IMAGE = 'test.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Segment Anything model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from os import getcwd, makedirs\n",
    "from os import path as osp\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import cv2 as cv\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from mlflow.models import ModelSignature, infer_signature\n",
    "from mlflow.pyfunc import PythonModel, log_model\n",
    "from mlflow.types.schema import ColSpec, DataType, Schema, TensorSpec\n",
    "from segment_anything import SamAutomaticMaskGenerator, SamPredictor, sam_model_registry\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "logs = logging.getLogger(\"mlflow\")\n",
    "# logs.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# Class to wrap Segment Anything Model\n",
    "class MLflowWrapperSAM(PythonModel):\n",
    "    # shape_input_sam: Tuple[int, int] = (800, 800)  # FIXME: set real SAM imput shape\n",
    "    min_mask_region_area: int = 100000\n",
    "\n",
    "    def __init__(self, model, shape_input=None, mask_min=None):\n",
    "        \"\"\"\n",
    "        Initializes the SAM wrapper.\n",
    "\n",
    "        :param model: Pre-loaded SAM model instance.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        # self.shape_input_sam = shape_input or self.shape_input_sam\n",
    "        self.min_mask_region_area = mask_min or self.min_mask_region_area\n",
    "\n",
    "        # Set up SAM Generator and Predictor\n",
    "        self.generator_sam = SamAutomaticMaskGenerator(self.model, min_mask_region_area=self.min_mask_region_area)\n",
    "        self.predictor_sam = SamPredictor(self.model)\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        # Read input data\n",
    "        images_base64 = model_input['image']\n",
    "        modes = model_input.get('mode', ['generator'] * len(images_base64))\n",
    "        pos_points = model_input.get('pos_points', [None] * len(images_base64))\n",
    "        neg_points = model_input.get('neg_points', [None] * len(images_base64))\n",
    "        bboxes = model_input.get('bboxes', [None] * len(images_base64))\n",
    "\n",
    "        # Debug messages\n",
    "        try:\n",
    "            logs.debug(\n",
    "                \"model input = %s\\n%s\", str(model_input.keys()), str({k: type(v) for k, v in model_input.items()})\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            logs.error(\"%s\", str(ex))\n",
    "        try:\n",
    "            logs.debug(\"image size = %s\", str([len(i) for i in images_base64]))\n",
    "        except Exception as ex:\n",
    "            logs.error(\"%s\", str(ex))\n",
    "        try:\n",
    "            logs.debug(\"mode = %s\", str([m for m in modes]))\n",
    "        except Exception as ex:\n",
    "            logs.error(\"%s\", str(ex))\n",
    "        try:\n",
    "            logs.debug(\"pos_points = %s\", str([pp for pp in pos_points]))\n",
    "        except Exception as ex:\n",
    "            logs.error(\"%s\", str(ex))\n",
    "        try:\n",
    "            logs.debug(\"neg_points = %s\", str([pn for pn in neg_points]))\n",
    "        except Exception as ex:\n",
    "            logs.error(\"%s\", str(ex))\n",
    "        try:\n",
    "            logs.debug(\"bboxes = %s\", str([bb for bb in bboxes]))\n",
    "        except Exception as ex:\n",
    "            logs.error(\"%s\", str(ex))\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for mode, image_base64, points_p, points_n, boxes in zip(modes, images_base64, pos_points, neg_points, bboxes):\n",
    "            predictions = []\n",
    "\n",
    "            # Decode image and convert to required format for generator\n",
    "            image_data = base64.b64decode(image_base64)\n",
    "            image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "            image = cv.imdecode(image_array, cv.IMREAD_COLOR)\n",
    "            input_image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "            # Debug messages\n",
    "            logs.debug(\"image data is %s\", str(type(image_data)))\n",
    "            logs.debug(\"image array is %s\", str(type(image_array)))\n",
    "            logs.debug(\"image is %s\", str(type(image)))\n",
    "            try:\n",
    "                logs.debug(\"image size = %s\", str(image.size))\n",
    "            except Exception as ex:\n",
    "                logs.error(\"%s\", str(ex))\n",
    "\n",
    "            # Check mode and apply appropriate function\n",
    "            if mode == 'generator':\n",
    "                # Run the generator mode\n",
    "                predictions = self.run_generator_mode(input_image)\n",
    "            elif mode == 'predictor':\n",
    "                # Run the predictor mode with points and bounding boxes.\n",
    "                # Assume that pos_points and neg_points are of type:\n",
    "                # np.array[[x, y], [x, y], ...]\n",
    "                input_points = np.array(points_p)  # if isinstance(points_p, (list, tuple)) else None\n",
    "                logs.debug(\n",
    "                    \"input positive points are %s\",\n",
    "                    str(input_points.shape if input_points is not None else type(input_points)),\n",
    "                )\n",
    "                if input_points is not None and input_points.shape[-1] == 2:\n",
    "                    input_points = input_points.reshape(-1, 2)\n",
    "                    input_labels = np.array(\n",
    "                        [1] * len(input_points)\n",
    "                    )  # if isinstance(input_points, (list, tuple)) else None\n",
    "                    logs.debug(\n",
    "                        \"input positive labels are %s\",\n",
    "                        str(input_labels.shape if input_labels is not None else type(input_labels)),\n",
    "                    )\n",
    "                    points_n = np.array(points_n)  # if isinstance(points_n, (list, tuple)) else None\n",
    "                    logs.debug(\n",
    "                        \"input negative points are %s\", str(points_n.shape if points_n is not None else type(points_n))\n",
    "                    )\n",
    "                    if points_n is not None and points_n.shape[-1] == 2:\n",
    "                        points_n = points_n.reshape(-1, 2)\n",
    "                        input_points = np.concatenate([input_points, points_n], axis=0)\n",
    "                        input_labels = np.concatenate([input_labels, np.array([0] * len(points_n))], axis=0)\n",
    "                        logs.debug(\n",
    "                            \"input positive and negative labels are %s\",\n",
    "                            str(input_labels.shape if input_labels is not None else type(input_labels)),\n",
    "                        )\n",
    "                else:\n",
    "                    logs.debug(\"no valid points found (predictor mode)! Skip...\")\n",
    "                    input_points = None\n",
    "                    input_labels = None\n",
    "\n",
    "                boxes = np.array(boxes)\n",
    "                logs.debug(\"boxes are %s\", str(boxes.shape if boxes is not None else type(boxes)))\n",
    "                predictions = self.run_predictor_mode(input_image, input_points, input_labels, boxes)\n",
    "                logs.debug(\"predictions are %s of size %d\", str(type(predictions)), len(predictions))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode specified. Use 'generator' or 'predictor'.\")\n",
    "\n",
    "            # Postprocess predictions\n",
    "            for prediction in predictions:\n",
    "                # Project predictions to original image\n",
    "                size_source = input_image.shape[1::-1]\n",
    "                size_target = prediction['crop_box'][2:4]\n",
    "                ratio = size_target[0] / size_source[0], size_target[1] / size_source[1]\n",
    "                # Project mask and its area\n",
    "                mask = np.array(prediction['segmentation'], dtype=np.uint8) * np.uint8(255)\n",
    "                mask = cv.resize(mask, size_source, interpolation=cv.INTER_NEAREST)\n",
    "                prediction['segmentation'] = mask  # push back resized mask to predictions\n",
    "                prediction['area'] = prediction['area'] * ratio[0] * ratio[1]  # update mask area\n",
    "                # Extra: replace mask with contours\n",
    "                polygon, hierarchy = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "                prediction['polygon'] = [p.tolist() for p in polygon]  # create polygon entry in predictions\n",
    "                logs.debug(\"polygon = %s\", str(polygon))\n",
    "                # prediction['segmentation'] = None  # delete raster masks from output\n",
    "                del prediction['segmentation']\n",
    "                # Project bbox\n",
    "                bbox = prediction['bbox']\n",
    "                bbox[0] = (bbox[0] + prediction['crop_box'][0]) / ratio[0]\n",
    "                bbox[1] = (bbox[1] + prediction['crop_box'][1]) / ratio[1]\n",
    "                bbox[2] /= ratio[0]\n",
    "                bbox[3] /= ratio[1]\n",
    "                prediction['bbox'] = bbox  # push bask resized bboxes to predictions\n",
    "                # Project points\n",
    "                logs.debug(\"point coords = %s\", str(prediction['point_coords']))\n",
    "                for point in prediction['point_coords']:\n",
    "                    # TODO: point relative to prediction['crop_box'] upper left point\n",
    "                    point[0] /= ratio[0]\n",
    "                    point[1] /= ratio[1]\n",
    "                prediction['logits'] = prediction.get('logits', None)  # default value if no exists\n",
    "            result.append(predictions)\n",
    "        return result\n",
    "\n",
    "    def run_generator_mode(self, input_image: np.ndarray) -> List[Dict[str, Any]]:\n",
    "        logs.debug(\"(generator mode): input image shape = %s\", str(input_image.shape))\n",
    "        masks = self.generator_sam.generate(input_image)\n",
    "        logs.debug(\"masks are %s\", str(type(masks)))\n",
    "        try:\n",
    "            logs.debug(\"masks = %s\", str([type(m) for m in masks]))\n",
    "        except Exception as ex:\n",
    "            logs.error(\"%s\", str(ex))\n",
    "        return masks  # return masks directly\n",
    "\n",
    "    def run_predictor_mode(self, input_image, points=None, labels=None, bboxes=None):\n",
    "        logs.debug(\"(predictor mode): input image shape = %s\", str(input_image.shape))\n",
    "        self.predictor_sam.set_image(input_image)\n",
    "\n",
    "        # Predict based on points and/or bounding boxes\n",
    "        masks = []\n",
    "        if points is not None:\n",
    "            if labels is None:\n",
    "                point_labels = np.array([1] * len(points))\n",
    "            else:\n",
    "                point_labels = labels\n",
    "            m, s, l = self.predictor_sam.predict(point_coords=points, point_labels=point_labels, multimask_output=False)\n",
    "            masks.extend(self.unify_predictor_output(m, s, l, points))\n",
    "        if bboxes is not None:\n",
    "            for bbox in bboxes:\n",
    "                m, s, l = self.predictor_sam.predict(box=bbox)\n",
    "                # Surrogate points from the center of the bbox\n",
    "                points = int((bbox[0] + bbox[2]) / 2), int((bbox[1] + bbox[3]) / 2)\n",
    "                masks.extend(self.unify_predictor_output(m, s, l, [points]))\n",
    "\n",
    "        return masks  # return masks in predictor mode\n",
    "\n",
    "    def unify_predictor_output(self, masks, scores, logits=None, point_coords=None):\n",
    "        \"\"\"\n",
    "        Converts the output of SamPredictor.predict to match the output format of SamAutomaticMaskGenerator.generate.\n",
    "\n",
    "        Args:\n",
    "            masks (np.ndarray): Array of predicted masks with shape (num_masks, height, width).\n",
    "            scores (np.ndarray): Array of confidence scores for each mask.\n",
    "            logits (np.ndarray, optional): Array of logits (may be used as inputs to SamPredictor.predict method).\n",
    "            point_coords (list(list(float)), optional): List of input point coordinates.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: Unified output format as a list of dictionaries matching SamAutomaticMaskGenerator.generate.\n",
    "        \"\"\"\n",
    "        unified_output = []\n",
    "\n",
    "        for i, mask in enumerate(masks):\n",
    "            # Calculate bounding box (bbox) for each mask\n",
    "            y_indices, x_indices = mask.nonzero()\n",
    "            if y_indices.size > 0 and x_indices.size > 0:\n",
    "                x_min, x_max = x_indices.min(), x_indices.max()\n",
    "                y_min, y_max = y_indices.min(), y_indices.max()\n",
    "                bbox = [int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)]\n",
    "            else:\n",
    "                bbox = [0, 0, 0, 0]\n",
    "            h, w = mask.shape[-2:]\n",
    "            crop_box = [0, 0, int(w), int(h)]\n",
    "\n",
    "            # Calculate area (number of non-zero pixels in the mask)\n",
    "            area = int(mask.sum())\n",
    "\n",
    "            # Add a placeholder for stability_score (can be computed more precisely if logits are available)\n",
    "            stability_score = float(scores[i])  # placeholder; use score as an approximate stability score\n",
    "            predicted_iou = 0.95  # TODO: approximate\n",
    "            try:\n",
    "                lrm = logits[i]\n",
    "            except Exception as ex:\n",
    "                logs.warning(\"%s\", str(ex))\n",
    "                lrm = None\n",
    "            point_coords_output = np.array(point_coords if point_coords is not None else [[0, 0]]).tolist()\n",
    "            logs.debug(\"unification points = %s\", str(point_coords_output))\n",
    "\n",
    "            # Append to output in unified format\n",
    "            unified_output.append(\n",
    "                {\n",
    "                    'segmentation': mask,  # binary mask for the object\n",
    "                    'bbox': bbox,  # bounding box in [x_min, y_min, width, height]\n",
    "                    'area': area,  # area of the mask in pixels\n",
    "                    'predicted_iou': predicted_iou,  # approximated with score\n",
    "                    'point_coords': point_coords_output,  # input points used for mask generation\n",
    "                    'stability_score': stability_score,  # placeholder stability score\n",
    "                    'crop_box': crop_box,  # crop box in [x_min, y_min, x_max, y_max]\n",
    "                    'logits': lrm,  # SamPredictor low-res masks used by CVAT\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return unified_output\n",
    "\n",
    "\n",
    "# Define the required environment packages\n",
    "conda_env_sam = {\n",
    "    'name': 'base',\n",
    "    'channels': ['conda-forge'],\n",
    "    'dependencies': [\n",
    "        'python=3.12',\n",
    "        'boto3',  # this may be required to download from S3\n",
    "        {\n",
    "            'pip': [\n",
    "                'opencv-python-headless',\n",
    "                '-f https://download.pytorch.org/whl/torch torch==2.5.0+cpu',\n",
    "                '-f https://download.pytorch.org/whl/torchvision torchvision==0.20.0+cpu',\n",
    "                'git+https://github.com/facebookresearch/segment-anything.git@6fdee8f2727f4506cfbbe553e23b895e27956588',\n",
    "            ]\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "mlflow.set_tracking_uri(f\"{MLFLOW_TRACKING_URI}\")\n",
    "mlflow.set_experiment(\"Segment-Anything\")\n",
    "\n",
    "\n",
    "# Define model URLs\n",
    "MODELS = {\n",
    "    'vit_h': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
    "    'vit_l': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
    "    'vit_b': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def get_model_weights(model_name, download_path='models'):\n",
    "    # Ensure the download directory exists\n",
    "    makedirs(download_path, exist_ok=True)\n",
    "\n",
    "    # Define the full path for the weights\n",
    "    weights_path = osp.join(download_path, osp.basename(MODELS[model_name]))\n",
    "\n",
    "    # Check if the file exists; if not, download it\n",
    "    if not osp.isfile(weights_path):\n",
    "        print(f\"Downloading {model_name} model weights...\")\n",
    "        urlretrieve(MODELS[model_name], weights_path)\n",
    "\n",
    "    return weights_path\n",
    "\n",
    "\n",
    "PREFIX_MODELS = osp.realpath(osp.join(getcwd(), 'models'))\n",
    "makedirs(PREFIX_MODELS, exist_ok=True)\n",
    "\n",
    "assert osp.isdir(PREFIX_MODELS), f\"check {PREFIX_MODELS} exists!\"\n",
    "\n",
    "NAME_PROJECT = 'MLflow'\n",
    "NAME_TASK = 'Segment-Anything'\n",
    "NAME_SUBTASK = 'SAM'\n",
    "\n",
    "models_registered = {}\n",
    "\n",
    "# Load and encode image to base64\n",
    "with open(PATH_IMAGE, 'rb') as image_file:\n",
    "    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Define the input schema\n",
    "input_schema = Schema.from_json(\n",
    "    '[{\"type\": \"string\", \"name\": \"mode\", \"required\": false}, {\"type\": '\n",
    "    '\"string\", \"name\": \"image\", \"required\": true}, {\"type\": \"array\", '\n",
    "    '\"items\": {\"type\": \"array\", \"items\": {\"type\": \"long\"}}, \"name\": '\n",
    "    '\"bboxes\", \"required\": false}, {\"type\": \"array\", \"items\": {\"type\": '\n",
    "    '\"array\", \"items\": {\"type\": \"long\"}}, \"name\": \"pos_points\", '\n",
    "    '\"required\": false}, {\"type\": \"array\", \"items\": {\"type\": \"array\", '\n",
    "    '\"items\": {\"type\": \"long\"}}, \"name\": \"neg_points\", \"required\": '\n",
    "    'false}]'\n",
    ")\n",
    "\n",
    "# Define the output schema\n",
    "output_schema = Schema.from_json(\n",
    "    '[{\"type\": \"array\", \"items\": {\"type\": \"object\", \"properties\": '\n",
    "    '{\"area\": {\"type\": \"double\", \"required\": true}, \"bbox\": {\"type\": '\n",
    "    '\"array\", \"items\": {\"type\": \"double\"}, \"required\": true}, '\n",
    "    '\"crop_box\": {\"type\": \"array\", \"items\": {\"type\": \"long\"}, '\n",
    "    '\"required\": true}, \"logits\": {\"type\": \"array\", \"items\": {\"type\": '\n",
    "    '\"array\", \"items\": {\"type\": \"float\"}}, \"required\": false}, '\n",
    "    '\"point_coords\": {\"type\": \"array\", \"items\": {\"type\": \"array\", '\n",
    "    '\"items\": {\"type\": \"double\"}}, \"required\": true}, \"polygon\": '\n",
    "    '{\"type\": \"array\", \"items\": {\"type\": \"array\", \"items\": {\"type\": '\n",
    "    '\"array\", \"items\": {\"type\": \"array\", \"items\": {\"type\": \"long\"}}}}, '\n",
    "    '\"required\": true}, \"predicted_iou\": {\"type\": \"double\", '\n",
    "    '\"required\": true}, \"stability_score\": {\"type\": \"double\", '\n",
    "    '\"required\": true}}}, \"required\": true}]'\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "# signature = None\n",
    "\n",
    "# Prepare the model input example\n",
    "# data = {'mode': 'generator', 'image': image_base64}\n",
    "data = {\n",
    "    'mode': 'predictor',\n",
    "    'image': image_base64,\n",
    "    'bboxes': [[200, 200, 600, 600]],\n",
    "    'pos_points': [[380, 380], [420, 420]],\n",
    "    'neg_points': [[180, 180], [620, 620]],\n",
    "}\n",
    "\n",
    "for ms in ('b', 'l', 'h'):\n",
    "    # Iterate over model sizes\n",
    "    NAME_VERSION = f\"{ms}+cpu\"\n",
    "    NAME_MODEL_SAM = f\"{NAME_PROJECT}-{NAME_TASK}-{NAME_SUBTASK}-{NAME_VERSION}\"\n",
    "\n",
    "    PATH_ARTIFACT = 'segment-anything-sam'\n",
    "\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    model_name = f\"vit_{ms}\"\n",
    "    path_model_weights = get_model_weights(model_name, PREFIX_MODELS)  # download weights\n",
    "\n",
    "    # Instantiate the model with weights and wrap it for MLflow\n",
    "    model = sam_model_registry[model_name](checkpoint=path_model_weights)\n",
    "    wrapped_model = MLflowWrapperSAM(model)\n",
    "\n",
    "    # raise KeyboardInterrupt\n",
    "    with mlflow.start_run(run_name=f\"{NAME_MODEL_SAM}-{TIMESTAMP}\"):\n",
    "        log_model(\n",
    "            artifact_path=PATH_ARTIFACT,\n",
    "            python_model=wrapped_model,\n",
    "            conda_env=conda_env_sam,\n",
    "            signature=signature,\n",
    "            input_example=data,\n",
    "        )\n",
    "        mlflow.set_tags({'project': NAME_PROJECT, 'task': NAME_TASK, 'model': NAME_SUBTASK, 'version': NAME_VERSION})\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "    model_uri = f\"runs:/{run_id}/{PATH_ARTIFACT}\"\n",
    "    models_registered[model_name] = mlflow.register_model(model_uri, NAME_MODEL_SAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check logged models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MODEL_SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.pyfunc import load_model\n",
    "\n",
    "\n",
    "model = load_model(f\"models:/{NAME_MODEL_SAM}/latest\")\n",
    "\n",
    "print(model.metadata.signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(model.metadata.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferred signatures\n",
    "\n",
    "As Python `object`:\n",
    "```\n",
    "inputs: \n",
    "  ['mode': string (optional), 'image': string (required), 'bboxes': Array(Array(long)) (optional), 'pos_points': Array(Array(long)) (optional), 'neg_points': Array(Array(long)) (optional)]\n",
    "outputs: \n",
    "  [Array({area: double (required), bbox: Array(double) (required), crop_box: Array(long) (required), logits: Array(Array(float)) (optional), point_coords: Array(Array(double)) (required), polygon: Array(Array(Array(Array(long)))) (required), predicted_iou: double (required), stability_score: double (required)}) (required)]\n",
    "params: \n",
    "  None\n",
    "```\n",
    "\n",
    "As Python `dict`:\n",
    "```python\n",
    "{'inputs': '[{\"type\": \"string\", \"name\": \"mode\", \"required\": false}, {\"type\": '\n",
    "           '\"string\", \"name\": \"image\", \"required\": true}, {\"type\": \"array\", '\n",
    "           '\"items\": {\"type\": \"array\", \"items\": {\"type\": \"long\"}}, \"name\": '\n",
    "           '\"bboxes\", \"required\": false}, {\"type\": \"array\", \"items\": {\"type\": '\n",
    "           '\"array\", \"items\": {\"type\": \"long\"}}, \"name\": \"pos_points\", '\n",
    "           '\"required\": false}, {\"type\": \"array\", \"items\": {\"type\": \"array\", '\n",
    "           '\"items\": {\"type\": \"long\"}}, \"name\": \"neg_points\", \"required\": '\n",
    "           'false}]',\n",
    " 'outputs': '[{\"type\": \"array\", \"items\": {\"type\": \"object\", \"properties\": '\n",
    "            '{\"area\": {\"type\": \"double\", \"required\": true}, \"bbox\": {\"type\": '\n",
    "            '\"array\", \"items\": {\"type\": \"double\"}, \"required\": true}, '\n",
    "            '\"crop_box\": {\"type\": \"array\", \"items\": {\"type\": \"long\"}, '\n",
    "            '\"required\": true}, \"logits\": {\"type\": \"array\", \"items\": {\"type\": '\n",
    "            '\"array\", \"items\": {\"type\": \"float\"}}, \"required\": true}, '\n",
    "            '\"point_coords\": {\"type\": \"array\", \"items\": {\"type\": \"array\", '\n",
    "            '\"items\": {\"type\": \"double\"}}, \"required\": true}, \"polygon\": '\n",
    "            '{\"type\": \"array\", \"items\": {\"type\": \"array\", \"items\": {\"type\": '\n",
    "            '\"array\", \"items\": {\"type\": \"array\", \"items\": {\"type\": \"long\"}}}}, '\n",
    "            '\"required\": true}, \"predicted_iou\": {\"type\": \"double\", '\n",
    "            '\"required\": true}, \"stability_score\": {\"type\": \"double\", '\n",
    "            '\"required\": true}}}, \"required\": true}]',\n",
    " 'params': None}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "\n",
    "# pprint(model.metadata.signature.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop here before continue with MLflow serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with MLflow serving\n",
    "\n",
    "MLflow serving must be started manually on any suitable machine. Let's follow some key assumptions:\n",
    "\n",
    "1. MLflow serving is going to be deployed on localhost\n",
    "2. MLflow tracking server is a separate host (not localhost)\n",
    "3. MLflow artifact storage is an S3-compatible storage (bucket)\n",
    "4. Variables `MLFLOW_TRACKING_URI`, `MLFLOW_S3_ENDPOINT_URL`, `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are set in `.env` file\n",
    "\n",
    "**Shell script example** (make sure there only variables `VARIABLE=value`):\n",
    "\n",
    "```shell\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# Check if .env file exists\n",
    "if [ ! -f .env ]; then\n",
    "    echo \"Error: .env file not found!\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Load environment variables from .env file\n",
    "while IFS= read -r line; do\n",
    "    # Skip empty lines and comments\n",
    "    [[ -z \"$line\" || \"$line\" =~ ^# ]] && continue\n",
    "    # Export the variable\n",
    "    export \"$line\"\n",
    "done < .env\n",
    "\n",
    "# Default model name\n",
    "MODEL=\"MLflow-Segment-Anything-SAM-b+cpu/latest\"\n",
    "# First argument is a model name\n",
    "MODEL=${1:-$MODEL}\n",
    "# Second argument is a port number\n",
    "PORT=${2:-10055}\n",
    "\n",
    "# Env manager set to `local` implies that MLflow will not try\n",
    "# to create any enviroment, but use local active environment (conda, pyenv, venv, etc)\n",
    "mlflow models serve -m \"models:/$MODEL\" -p \"$PORT\" --env-manager local --timeout 1440\n",
    "```\n",
    "\n",
    "Assume script name is `start-mlflow`:\n",
    "\n",
    "```shell\n",
    "chmod +x start-mlflow\n",
    "\n",
    "./start-mlflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'NAME_MODEL_SAM' not in locals():\n",
    "    NAME_MODEL_SAM = 'MLflow-Segment-Anything-SAM-b+cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Mask Generator\n",
    "\n",
    "Predict using SAM Automatic Mask Generation mode.\n",
    "\n",
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# PATH_IMAGE = 'test.jpg'\n",
    "\n",
    "# Load and encode image to base64\n",
    "with open(PATH_IMAGE, 'rb') as image_file:\n",
    "    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Prepare the payload\n",
    "data = {'mode': 'generator', 'image': image_base64}\n",
    "payload = {'instances': [data]}\n",
    "\n",
    "# Send the request to the MLflow serving endpoint\n",
    "url = 'http://localhost:10055/invocations'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "This is an example how to interpret MLflow SAM serving results (generator mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Check if response is successful\n",
    "if response.status_code == 200:\n",
    "    # Decode response\n",
    "    response_data = response.json()\n",
    "    if isinstance(response_data, dict) and 'predictions' in response_data:\n",
    "        for predictions in response_data['predictions']:\n",
    "            # List response_data['predictions'] is a batch of images from payload['instances']\n",
    "            image = plt.imread(PATH_IMAGE).copy()\n",
    "            for prediction in predictions:\n",
    "                color = (randint(15, 240), randint(15, 240), randint(15, 240))\n",
    "                polygon = prediction['polygon']\n",
    "                image = cv.drawContours(image, tuple(map(np.array, polygon)), -1, color, 2, cv.LINE_AA)\n",
    "                for point in prediction['point_coords']:\n",
    "                    image = cv.circle(image, tuple(map(round, point)), 3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "                bbox = prediction['bbox']\n",
    "                p1 = round(bbox[0]), round(bbox[1])\n",
    "                p2 = round(bbox[0] + bbox[2]), round(bbox[1] + bbox[3])\n",
    "                image = cv.rectangle(image, p1, p2, (5, 5, 5), 1, cv.LINE_AA)\n",
    "            plt.figure(figsize=(1280 / 72, 960 / 72), dpi=72)\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"Unexpected response format: {type(response_data)}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(response_data['predictions'][0][0].keys())\n",
    "except:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor\n",
    "\n",
    "Predict using SAM Prediction mode.\n",
    "\n",
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# PATH_IMAGE = 'test.jpg'\n",
    "\n",
    "# Load and encode image to base64\n",
    "with open(PATH_IMAGE, 'rb') as image_file:\n",
    "    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Prepare the payload\n",
    "data = {\n",
    "    'mode': 'predictor',\n",
    "    'image': image_base64,\n",
    "    'bboxes': [[200, 200, 600, 600]],\n",
    "    'pos_points': [[380, 380], [420, 420]],\n",
    "    'neg_points': [[180, 180], [620, 620]],\n",
    "}\n",
    "payload = {'instances': [data]}\n",
    "\n",
    "# Send the request to the MLflow serving endpoint\n",
    "url = 'http://localhost:10055/invocations'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "This is an example how to interpret MLflow SAM serving results (generator mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Check if response is successful\n",
    "if response.status_code == 200:\n",
    "    # Decode response\n",
    "    response_data = response.json()\n",
    "    if isinstance(response_data, dict) and 'predictions' in response_data:\n",
    "        for predictions in response_data['predictions']:\n",
    "            # List response_data['predictions'] is a batch of images from payload['instances']\n",
    "            image = plt.imread(PATH_IMAGE).copy()\n",
    "            for prediction in predictions:\n",
    "                color = (randint(15, 240), randint(15, 240), randint(15, 240))\n",
    "                polygon = prediction['polygon']\n",
    "                image = cv.drawContours(image, tuple(map(np.array, polygon)), -1, color, 2, cv.LINE_AA)\n",
    "                for point in prediction['point_coords']:\n",
    "                    image = cv.circle(image, tuple(map(round, point)), 3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "                bbox = prediction['bbox']\n",
    "                p1 = round(bbox[0]), round(bbox[1])\n",
    "                p2 = round(bbox[0] + bbox[2]), round(bbox[1] + bbox[3])\n",
    "                image = cv.rectangle(image, p1, p2, (5, 5, 5), 1, cv.LINE_AA)\n",
    "            plt.figure(figsize=(1280 / 72, 960 / 72), dpi=72)\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"Unexpected response format: {type(response_data)}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(response_data['predictions'][0][0].keys())\n",
    "except:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
